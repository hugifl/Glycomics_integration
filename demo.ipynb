{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T15:43:13.106269Z",
     "start_time": "2020-07-10T15:43:13.099377Z"
    }
   },
   "outputs": [],
   "source": [
    "cd ~/git/scim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T16:06:33.557324Z",
     "start_time": "2020-07-10T16:06:33.538188Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import anndata\n",
    "from itertools import cycle\n",
    "\n",
    "import scanpy as sc\n",
    "from pathlib import Path\n",
    "\n",
    "from scim.model import VAE, Integrator\n",
    "from scim.discriminator import SpectralNormCritic\n",
    "from scim.trainer import Trainer\n",
    "from scim.utils import switch_obsm, make_network, plot_training, adata_to_pd\n",
    "from scim.simulate import simulate\n",
    "from scim.evaluate import score_divergence, extract_matched_labels, get_accuracy, get_confusion_matrix\n",
    "from scim.matching import get_cost_knn_graph, mcmf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T16:17:07.568042Z",
     "start_time": "2020-07-10T16:17:07.562966Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTDIR = Path('./.cache/demo')\n",
    "SEED = 12\n",
    "TECHS = ['source', 'target']\n",
    "LABEL = 'branch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROSSTT params\n",
    "ngenes = 200\n",
    "ncells = 20000\n",
    "tree = '(B:50,C:50)A:50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and cache data\n",
    "\n",
    "cache = {key: OUTDIR.joinpath('0_data', f'{key}.h5ad') for key in TECHS}\n",
    "\n",
    "train = dict()\n",
    "test = dict()\n",
    "full = dict()\n",
    "\n",
    "try:\n",
    "    for tech in TECHS:\n",
    "        path = cache[tech]\n",
    "        data = anndata.read(path)\n",
    "        full[tech] = data\n",
    "        print(f'Loaded {path}')\n",
    "        \n",
    "except OSError:\n",
    "    print('Error loading, recomputing data')\n",
    "    np.random.seed(SEED)\n",
    "    for tech in TECHS:\n",
    "        data = simulate(tree, ngenes, ncells)\n",
    "        full[tech] = data\n",
    "        \n",
    "        path = cache[tech]\n",
    "        data.write(cache[tech])\n",
    "        print(f'Cached {path}')\n",
    "\n",
    "\n",
    "for tech, data in full.items():\n",
    "    train[tech] = data[data.obs['is_train']]\n",
    "    test[tech] = data[~data.obs['is_train']]\n",
    "\n",
    "# Convert train into tf.data.Datasets\n",
    "for tech, val in train.items():\n",
    "    train[tech] = tf.data.Dataset.from_tensor_slices((val.X, val.obs[LABEL].cat.codes, val.obs_names.astype(int)))\n",
    "    train[tech] = train[tech].batch(256)\n",
    "\n",
    "# Need to know what the labels are\n",
    "label_cats = None\n",
    "for data in full.values():    \n",
    "    # assert label categories is same on all technologies\n",
    "    if label_cats is None:\n",
    "        label_cats = data.obs[LABEL].cat.categories\n",
    "    else:\n",
    "        assert label_cats.equals(data.obs[LABEL].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize simulated data\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for idx, key in enumerate(TECHS):\n",
    "    for jdx, color in enumerate(['branch', 'pt']):\n",
    "        ax = axes[idx, jdx]\n",
    "        sc.pl.pca(full[key], color=color, ax=ax, show=False)\n",
    "        ax.set_title(f'{key} {color}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "latent_dim = 8\n",
    "\n",
    "discriminator_net = make_network(\n",
    "    doutput=1,\n",
    "    units=[8]*2,\n",
    "    dinput=latent_dim + label_cats.size,\n",
    "    batch_norm=False,\n",
    "    name='discriminator')\n",
    "\n",
    "discriminator = SpectralNormCritic(\n",
    "    discriminator_net,\n",
    "    input_cats=label_cats)\n",
    "\n",
    "vae_lut = dict()\n",
    "for tech in TECHS:\n",
    "    encoder = make_network(\n",
    "        doutput=2*latent_dim, \n",
    "        units=[32]*2,\n",
    "        dinput=ngenes,\n",
    "        batch_norm=True, dropout=0.2,\n",
    "        name=f'{tech}-encoder')\n",
    "\n",
    "    decoder = make_network(\n",
    "        doutput=ngenes,\n",
    "        units=[32]*2,\n",
    "        dinput=latent_dim,\n",
    "        batch_norm=True, dropout=0.2,\n",
    "        name=f'{tech}-decoder')\n",
    "\n",
    "    vae_lut[tech] = VAE(\n",
    "        encoder_net=encoder,\n",
    "        decoder_net=decoder,\n",
    "        name=f'{tech}-vae')\n",
    "    \n",
    "genopts = {key: tf.keras.optimizers.Adam() for key in TECHS}\n",
    "disopt = tf.keras.optimizers.Adam()\n",
    "\n",
    "trainer = Trainer(\n",
    "    vae_lut=vae_lut,\n",
    "    discriminator=discriminator,\n",
    "    source_key='source',\n",
    "    disopt=disopt,\n",
    "    genopt_lut=genopts,\n",
    "    beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the latent space by training a VAE on source technology\n",
    "\n",
    "ckpt_dir = OUTDIR.joinpath('1_init', 'model')\n",
    "\n",
    "\n",
    "source = 'source'\n",
    "target = 'target'\n",
    "\n",
    "try:\n",
    "    trainer.restore(ckpt_dir)\n",
    "\n",
    "except AssertionError:\n",
    "    print('Initializing latent space by training VAE')\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    tf.compat.v1.random.set_random_seed(SEED)\n",
    "\n",
    "    gs = 0\n",
    "    for epoch in range(10):\n",
    "        for (data, dlabel, _), (prior, plabel, _) in zip(train[source], cycle(train[target])):\n",
    "\n",
    "            # Initializes latent space\n",
    "            loss, (mse, kl), (codes, recon) = trainer.vae_step(source, data, beta=0.001)\n",
    "\n",
    "            # Initialize the discriminator\n",
    "            disc_loss, _ = trainer.discriminator_step(source, target, data, prior, dlabel, plabel)\n",
    "\n",
    "            # Record\n",
    "            if gs % 10 == 0:\n",
    "                lut = {'loss': loss.numpy().mean(),\n",
    "                       'mse': mse.numpy().mean(),\n",
    "                       'kl': kl.numpy().mean()}\n",
    "                trainer.record('vae', lut, step=gs)\n",
    "\n",
    "            gs = gs + 1\n",
    "\n",
    "    trainer.saver.save(ckpt_dir.joinpath('ckpt'))\n",
    "\n",
    "    # Plot training curve\n",
    "    fig, ax = plt.subplots()\n",
    "    for key, pairs in trainer.history['vae'].items():\n",
    "        step, vals = list(zip(*pairs))\n",
    "        ax.plot(step, vals, label=key)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 2)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print('Loaded checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the initialized latent space\n",
    "\n",
    "path = OUTDIR.joinpath('1_init', 'source_codes.h5ad')\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "try:\n",
    "    codes = anndata.read(path)\n",
    "    print(f'Loaded from cache {path}')\n",
    "    \n",
    "except OSError:\n",
    "    print('Cache loading failed')\n",
    "    \n",
    "    trainer.forward('source', full['source'], LABEL)\n",
    "    codes = switch_obsm(full['source'], 'code')\n",
    "    sc.tl.pca(codes)\n",
    "    sc.tl.tsne(codes, n_jobs=5)\n",
    "    codes.write(path)\n",
    "    print(f'Caching to {path}')\n",
    "    \n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for idx, key in enumerate(['branch', 'pt']):\n",
    "    sc.pl.pca(codes, color=key, ax=axes[0, idx], show=False)\n",
    "    sc.pl.tsne(codes, color=key, ax=axes[1, idx], show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize discriminator performance on non-aligned latent space\n",
    "path = OUTDIR.joinpath('1_init', 'evals.h5ad')\n",
    "path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "try:\n",
    "    evald = anndata.read(path)\n",
    "    print(f'Loaded from cache {path}')\n",
    "    \n",
    "except OSError:\n",
    "    print('Cache loading failed')\n",
    "    evald = trainer.evaluate(test, LABEL)\n",
    "    sc.tl.tsne(evald, n_jobs=5)\n",
    "    evald.write(path)\n",
    "    print(f'Caching to {path}')\n",
    "    \n",
    "sc.pl.tsne(evald, color='probs-discriminator', color_map='PiYG', vmin=0, vmax=1)\n",
    "sc.pl.tsne(evald, color='tech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Integrate target to source codes\n",
    "# This might take 5-10 mins\n",
    "\n",
    "%%time\n",
    "\n",
    "ckpt_dir = OUTDIR.joinpath('2_integrate', 'model')\n",
    "\n",
    "source = 'target'\n",
    "target = 'source'\n",
    "try:\n",
    "    trainer.restore(ckpt_dir)\n",
    "\n",
    "except AssertionError:\n",
    "    print('Training integration')\n",
    "    np.random.seed(SEED)\n",
    "    tf.compat.v1.random.set_random_seed(SEED)\n",
    "    \n",
    "    gs = 0\n",
    "    for epoch in range(25):\n",
    "        for (data, dlabel, didx), (prior, plabel, pidx) in zip(train[source], cycle(train[target])):\n",
    "\n",
    "            # Train the discriminator\n",
    "            for _ in range(trainer.niters_discriminator):\n",
    "                disc_loss, _ = trainer.discriminator_step(source, target, data, prior, dlabel, plabel)\n",
    "\n",
    "            # Fool adversary & reconstruct\n",
    "            loss, (nll, adv), (codes, recon) = trainer.adversarial_step(source, data, dlabel)\n",
    "\n",
    "            # Evaluate training batch, write to trainer.history\n",
    "            if gs % 5 == 0:\n",
    "                \n",
    "                batch = full[source][didx.numpy()]\n",
    "                trainer.forward(source, batch, LABEL)\n",
    "\n",
    "                pbatch = full[target][pidx.numpy()]\n",
    "                trainer.forward(target, pbatch, LABEL)\n",
    "\n",
    "                lut = {f'mse-{source}': batch.obs['loss-mse'].mean(),\n",
    "                       f'probs-{source}': batch.obs['probs-discriminator'].mean(),\n",
    "                       f'probs-{target}': pbatch.obs['probs-discriminator'].mean(),\n",
    "                       f'discriminator-{source}': batch.obs['loss-discriminator'].mean(),\n",
    "                       f'discriminator-{target}': pbatch.obs['loss-discriminator'].mean(),\n",
    "                      }\n",
    "                trainer.record(f'train', lut, gs)\n",
    "\n",
    "            # Evaluate testset\n",
    "            if gs % 50 == 0:\n",
    "                evald = trainer.evaluate(test, LABEL)\n",
    "                probs = evald.obs.groupby('tech')['probs-discriminator'].mean()\n",
    "                dloss = evald.obs.groupby('tech')['loss-discriminator'].mean()\n",
    "                mse = evald.obs.groupby('tech')['loss-mse'].mean()\n",
    "\n",
    "                lut = {'divergence': evald.uns['divergence']}\n",
    "                lut.update({f'probs-{k}': v for k, v in probs.to_dict().items()})\n",
    "                lut.update({f'mse-{k}': v for k, v in mse.to_dict().items()})\n",
    "                lut.update({f'discriminator-{k}': v for k, v in dloss.to_dict().items()})\n",
    "\n",
    "                trainer.record('test', lut, gs)\n",
    "\n",
    "            gs = gs + 1\n",
    "\n",
    "        trainer.saver.save(ckpt_dir.joinpath('ckpt'))\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(5, 9))\n",
    "        plot_training(trainer, axes)\n",
    "        \n",
    "else:\n",
    "    print('Loaded checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated latent space\n",
    "\n",
    "cache = OUTDIR.joinpath('2_integrate', 'evals.h5ad')\n",
    "try:\n",
    "    evald = anndata.read(cache)\n",
    "    print(f'Read from {cache}')\n",
    "    \n",
    "except OSError:\n",
    "    print('Cache loading failed')\n",
    "    evald = trainer.evaluate(test, LABEL)\n",
    "    sc.tl.pca(evald)\n",
    "    sc.tl.tsne(evald, n_jobs=5)\n",
    "    \n",
    "    evald.write(cache)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8), gridspec_kw={'wspace':0.5})\n",
    "for color, ax in zip(['tech', 'branch', 'probs-discriminator', 'pt'], axes.ravel()):\n",
    "    kwargs = {}\n",
    "    if color == 'probs-discriminator':\n",
    "        kwargs = {'color_map': 'PiYG', 'vmin':0, 'vmax':1}\n",
    "    sc.pl.tsne(evald, color=color, ax=ax, show=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize integrated latent space\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for key, axl in zip(TECHS, axes):\n",
    "    mask = evald.obs['tech'] == key\n",
    "    for color, ax in zip(['branch', 'pt'], axl):\n",
    "        sc.pl.tsne(evald[mask], color=color, ax=ax, show=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache model outputs\n",
    "trainer.forward(source, full[source], LABEL)\n",
    "trainer.forward(target, full[target], LABEL)\n",
    "\n",
    "scode = switch_obsm(full[source], 'code')\n",
    "tcode = switch_obsm(full[target], 'code')\n",
    "inter = scode.concatenate(tcode,\n",
    "                          batch_categories=[source, target],\n",
    "                          batch_key='tech')\n",
    "\n",
    "\n",
    "full[source].write(OUTDIR.joinpath('2_integrate', f'{source}.h5ad'))\n",
    "full[target].write(OUTDIR.joinpath('2_integrate', f'{target}.h5ad'))\n",
    "inter.write(OUTDIR.joinpath('2_integrate', f'codes.h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T16:29:20.148469Z",
     "start_time": "2020-07-10T16:18:29.145358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform bipartite matching on the latent embeddings\n",
    "source_pd = adata_to_pd(scode, add_cell_code_name='source')\n",
    "target_pd = adata_to_pd(tcode, add_cell_code_name='target')\n",
    "\n",
    "# Build an extended knn graph with k = 10% cells\n",
    "G = get_cost_knn_graph(source_pd, target_pd, knn_k=20, null_cost_percentile=95, capacity_method='uniform')\n",
    "\n",
    "# Run mcmf and extract matches\n",
    "row_ind, col_ind = mcmf(G)\n",
    "matches = extract_matched_labels(scode.obs, tcode.obs, row_ind, col_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T16:29:20.194647Z",
     "start_time": "2020-07-10T16:29:20.151868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cache matches\n",
    "matches.to_csv(OUTDIR.joinpath('3_matching', f'k20_nullcost95_uniform.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T16:30:02.704254Z",
     "start_time": "2020-07-10T16:30:02.661022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate matches based on branch label\n",
    "accuracy = get_accuracy(matches, colname_compare='branch')\n",
    "print('accuracy (branch): ', accuracy)\n",
    "get_confusion_matrix(matches, colname_compare='branch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tpenv)",
   "language": "python",
   "name": "tpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
